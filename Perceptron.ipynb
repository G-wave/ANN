{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron is the first implmentation of neurual network, which is simple while also including most of important ideas, which is often be the first algorithms to learning in machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem setup: input data are 2 dimensions $x^{(i)} =\\small\\{x_1^{(i)}, x_2^{(i)}\\small\\};  y^{(i)} =\\small\\{1, -1\\small\\}$\n",
    "\n",
    "\n",
    "\n",
    "### Cost Function: $J = \\frac{1}{n} \\sum_{i=1}^{n}I(y^{(i)} \\neq f(x^{(i)})) = \\frac{1}{n} \\sum_{i=1}^{n}I(y^{(i)} \\neq \\theta x^{(i)} + \\theta_0) = \\frac{1}{n} \\sum_{i=1}^{n} I(y^{(i)} (\\theta x^{(i)} + \\theta_0) \\leq 0)$\n",
    "\n",
    "\n",
    "\n",
    "### Consider the case only 1 data example ($n=1$), which is the case of Stochastic Gradient:\n",
    "$\\nabla_{\\theta_i}J = \\frac {\\partial} {\\partial \\theta_i} y (\\theta x + \\theta_0) = \\frac {\\partial} {\\partial \\theta_i} y (\\theta x) =  \\frac {\\partial} {\\partial \\theta_i} y (\\theta_1 x_1 + \\theta_2 x_2) = yx_i$\n",
    "\n",
    "$\\nabla_{\\theta}J= yx$\n",
    "\n",
    "### Gradient Descent Weight Update: $\\theta = \\theta - \\eta \\nabla_{\\theta}J = \\theta - \\eta yx =  \\theta - yx $ ... where $\\eta =1$ and it show the basic perceptron update rule, many book show this rule directly and did not state where it comes from despite its very simple form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#+++++++++++++++++++++++++++++++++ regularization term pushs the boundary further away of decision bounday"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hinge Loss: $Loss_h(z) = \\bigg\\{ _{1-z  \\quad o.w.} ^{0 \\quad where \\hspace{2mm}  z\\ge1}$\n",
    "\n",
    "### Large Margin Cost Function: $J  = average \\hspace{1mm} cost + regularization = \\frac{1}{n} \\sum_{i=1}^{n} Loss_h[y^{(i)}(\\theta x^{(i)} + \\theta_0)] + \\frac{\\lambda}{2}\\theta^2$\n",
    "\n",
    "### Consider the case only 1 data example $(x^{(i)},y^{(i)})$ ($n=1$), which is the case of Stochastic Gradient:\n",
    "### $\\nabla_{\\theta_j}J = \\frac {\\partial} {\\partial \\theta_j} Loss_h[y^{(i)}(\\theta x^{(i)} + \\theta_0)] + \\frac{\\lambda}{2}\\theta^2 = \\bigg\\{ ^{\\lambda\\theta_j \\quad\\quad\\quad cost=0}_{-y^{(i)}x^{(i)} + \\lambda\\theta_j  \\quad cost\\leq0}$\n",
    "\n",
    "### $\\nabla_{\\theta}J= \\bigg\\{ ^{\\lambda\\theta \\quad\\quad\\quad cost=0}_{-y^{(i)}x^{(i)} + \\lambda\\theta  \\quad cost\\leq0}$\n",
    "\n",
    "### Weight Update: $\\theta = \\theta - \\eta \\nabla_{\\theta}J $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
