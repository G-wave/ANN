{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron is the earliest and simplest implementation of neural network and usually also the first algorithm to learn. It can solve any linear seperable problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Problem setup: n 2 dimensions data examples,  the i-th data example  $(x^{(i)}, y^{(i)})$, where  $x^{(i)} =\\small\\{x_1^{(i)}, x_2^{(i)}\\small\\};  y^{(i)} =\\small\\{1, -1\\small\\}$\n",
    "\n",
    "### Predicton: $y^{(i)} = sign(\\theta \\cdot x^{(i)} + \\theta_0)$\n",
    "\n",
    "\n",
    "\n",
    "### Cost Function (0/1): \n",
    "#### $J(\\theta, \\theta_0) = \\frac{1}{n} \\sum_{i=1}^{n}I(y^{(i)} \\neq f(x^{(i)})) = \\frac{1}{n} \\sum_{i=1}^{n}I(y^{(i)} \\neq \\theta x^{(i)} + \\theta_0) = \\frac{1}{n} \\sum_{i=1}^{n} I(y^{(i)} (\\theta x^{(i)} + \\theta_0) \\leq 0) = E [I(y (\\theta x + \\theta_0)) \\leq 0] = 1 \\cdot P(y (\\theta x + \\theta_0 )\\le0) + 0 \\cdot P(y (\\theta x + \\theta_0)>0) = P(y (\\theta x + \\theta_0) \\le0)$ \n",
    "$I()$ is indicator function, cost = 0 when classfiy correctly, otherwise cost =1.\n",
    "\n",
    "\n",
    "\n",
    "##### Consider the case only 1 data example ($n=1$), which is the case of SGD (Stochastic Gradient Descent) ,  Weight Update Rule: $\\theta =  \\theta + yx $, when the example is wrongly classfied. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#+++++++++++++++++++++++++++++++++ regularization term pushs the boundary further away of decision bounday"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hinge Loss: $Loss_h(z) = \\bigg\\{ _{1-z  \\quad o.w.} ^{0 \\quad where \\hspace{2mm}  z\\ge1}$\n",
    "\n",
    "### Large Margin Cost Function: $J  = average \\hspace{1mm} cost + regularization = \\frac{1}{n} \\sum_{i=1}^{n} Loss_h[y^{(i)}(\\theta x^{(i)} + \\theta_0)] + \\frac{\\lambda}{2}\\theta^2$\n",
    "\n",
    "### Consider the case only 1 data example $(x^{(i)},y^{(i)})$ ($n=1$), which is the case of Stochastic Gradient:\n",
    "### $\\nabla_{\\theta_j}J = \\frac {\\partial} {\\partial \\theta_j} Loss_h[y^{(i)}(\\theta x^{(i)} + \\theta_0)] + \\frac{\\lambda}{2}\\theta^2 = \\bigg\\{ ^{\\lambda\\theta_j \\quad\\quad\\quad cost=0}_{-y^{(i)}x^{(i)} + \\lambda\\theta_j  \\quad cost\\leq0}$\n",
    "\n",
    "### $\\nabla_{\\theta}J= \\bigg\\{ ^{\\lambda\\theta \\quad\\quad\\quad cost=0}_{-y^{(i)}x^{(i)} + \\lambda\\theta  \\quad cost\\leq0}$\n",
    "\n",
    "### Weight Update: $\\theta = \\theta - \\eta \\nabla_{\\theta}J $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
